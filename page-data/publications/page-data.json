{"componentChunkName":"component---src-pages-publications-index-js","path":"/publications/","result":{"data":{"allPublicationsJson":{"nodes":[{"id":"f8fffec9-b374-5835-8e85-0ec13324e466","authors":[{"id":"530344ac-fbae-5332-b90f-066e12de1a9b","name":"Francesco","surname":"Vigni","web":"https://scholar.google.com/citations?user=ksO3xN0AAAAJ&hl=en","slug":"francesco-vigni"},{"id":"8fbee7fe-3f6a-536e-8db2-71d4a70af0f8","name":"Silvia","surname":"Rossi","web":"https://scholar.google.com/citations?user=Q_F1-QIAAAAJ&hl=en&oi=ao","slug":"silvia-rossi"}],"abstract":"The growing deployment of robots in social contexts implies the need to model their behaviour as social agents. In this context, the way a robot approaches a user and eventually engages in an interaction is a crucial aspect to take into account for the acceptance of these tools.\n In this work, we explore how the approaching policy and gaze behaviours can influence the perceived intention to interact before the interaction starts. The conducted user study highlights the importance of the robot’s gaze behaviour when approaching a human with respect to its approaching behaviour. In particular, if the robot moves in the surroundings of a human, even not straightforward in their direction, but locks the gaze at them, the intention to interact is recognised clearer and faster with respect to the direct approaching of the user but with an adverse gaze.","slug":"icsr-approach","tags":["approaching","gaze","hri"],"title":"Exploring Non-Verbal Strategies for Initiating an HRI","url":"https://link.springer.com/chapter/10.1007/978-3-031-24667-8_25","venue":"2022 International Conference on Social Robotics (ICSR)","date":"Feb 2023"},{"id":"9e66b59b-c59d-5ee4-af0d-30ee72c87895","authors":[{"id":"530344ac-fbae-5332-b90f-066e12de1a9b","name":"Francesco","surname":"Vigni","web":"https://scholar.google.com/citations?user=ksO3xN0AAAAJ&hl=en","slug":"francesco-vigni"},{"id":"66cc5e18-c2d7-560d-a550-355e5fa74476","name":"Alessandra","surname":"Rossi","web":"https://scholar.google.com/citations?hl=en&user=ETBNEJ0AAAAJ","slug":"alessandra-rossi"},{"id":"ca3ba40c-746d-5619-8c58-da410e7fd96b","name":"Linda","surname":"Miccio","web":null,"slug":"linda-miccio"},{"id":"8fbee7fe-3f6a-536e-8db2-71d4a70af0f8","name":"Silvia","surname":"Rossi","web":"https://scholar.google.com/citations?user=Q_F1-QIAAAAJ&hl=en&oi=ao","slug":"silvia-rossi"}],"abstract":"Non-anthropomorphic robots have issues in conveying internal state during a Human-Robot Interaction (HRI). A possible approach is to let robots communicate their states or intentions through emotions. However, the robot’s emotional responses are not always clearly identified by people, and it is also difficult to identify which and how many cues are most relevant in affecting people’s ability of recognition of robots’ emotions during the ongoing interaction. We involved 102 participants in an online questionnaire-based study where they rated the robot’s behaviours, designed in terms of colours, movements and sounds, according to the perceived emotions in order to identify the cues to be used for making robots more legible. The results suggest that emotional transparency can benefit from multimodal interaction. Our results underline that single modes can be capable of conveying effectively the desired emotion, and little benefit is obtained by the use of additional modes that may be not necessarily noticed by the users.","slug":"icsr-emotions","tags":["emotion","hri"],"title":"On The Emotional Transparency of a Non-Humanoid Social Robot","url":"https://link.springer.com/chapter/10.1007/978-3-031-24667-8_26","venue":"2022 International Conference on Social Robotics (ICSR)","date":"Feb 2023"},{"id":"d889cadc-54e5-5ff5-b7b9-4976e60262ac","authors":[{"id":"fd22eb6f-1c56-52c1-8a88-91b404168e24","name":"Georgios","surname":"Angelopoulos","web":"https://scholar.google.com/citations?hl=en&user=C2jDeBkAAAAJ","slug":"georgios-angelopoulos"},{"id":"530344ac-fbae-5332-b90f-066e12de1a9b","name":"Francesco","surname":"Vigni","web":"https://scholar.google.com/citations?user=ksO3xN0AAAAJ&hl=en","slug":"francesco-vigni"},{"id":"66cc5e18-c2d7-560d-a550-355e5fa74476","name":"Alessandra","surname":"Rossi","web":"https://scholar.google.com/citations?hl=en&user=ETBNEJ0AAAAJ","slug":"alessandra-rossi"},{"id":"84456754-6387-59b9-9474-36214e44e4a1","name":"Giuseppina","surname":"Russo","web":null,"slug":"giuseppina-russo"},{"id":"418cd9e7-749a-5b75-911d-311dcd1efe22","name":"Mario","surname":"Turco","web":null,"slug":"mario-turco"},{"id":"8fbee7fe-3f6a-536e-8db2-71d4a70af0f8","name":"Silvia","surname":"Rossi","web":"https://scholar.google.com/citations?user=Q_F1-QIAAAAJ&hl=en&oi=ao","slug":"silvia-rossi"}],"abstract":"When navigating in a shared environment, the extent to which robots are able to effectively use signals for coordinating with human behaviors can ameliorate dissatisfaction and increase acceptance. In this paper, we present an online video study to investigate whether familiar acoustic signals can improve the legibility of a robot’s navigation behavior. We collected the responses of 120 participants to evaluate their perceptions of a robot that communicates with one of the three used non-verbal navigational cues (an acoustic signal, an acoustic in pair with a visual signal, and a dissimilar frequency acoustic signal). Our results showed a significant legibility improvement when the robot used both light and acoustic signals to communicate its intentions compared to using only the same acoustic sound. Additionally, our findings highlighted that people also perceived differently the robot’s intentions when they were expressed through two frequencies of the mere sound. The results of this work suggest a paradigm that can help the development of mobile service robots in public spaces.","slug":"roman-roomba","tags":["acustic cues","roman"],"title":"Familiar Acoustic Cues for Legible Service Robots","url":"https://ieeexplore.ieee.org/document/9900699","venue":"2022 IEEE International Conference on Robot and Human Interactive (RO-MAN)","date":"Aug 2022"},{"id":"04dffc2b-2171-590e-ab4a-7e97fcdc7026","authors":[{"id":"530344ac-fbae-5332-b90f-066e12de1a9b","name":"Francesco","surname":"Vigni","web":"https://scholar.google.com/citations?user=ksO3xN0AAAAJ&hl=en","slug":"francesco-vigni"},{"id":"d2703613-4de5-5cca-8746-e3d741cc5d92","name":"Espen","surname":"Knoop","web":null,"slug":"espen-knopp"},{"id":"97eb6e91-d8fb-58fa-8c20-9453cc47f704","name":"Domenico","surname":"Prattichizzo","web":"https://scholar.google.com/citations?hl=en&user=mMjWcPAAAAAJ","slug":"domenico-prattichizzo"},{"id":"ae72bbfe-a006-5654-bfd1-64ad3d9f7d2d","name":"Monica","surname":"Malvezzi","web":"https://scholar.google.com/citations?user=M0NWWgcAAAAJ&hl=en","slug":"monica-malvezzi"}],"abstract":"In this letter, we investigate the role of haptic feedback in human–robot handshaking by comparing different force controllers. The basic hypothesis is that in human handshaking force control there is a balance between an intrinsic (open-loop) and extrinsic (closed-loop) contributions. We use an underactuated anthropomorphic robotic hand, the Pisa/IIT hand, instrumented with a set of pressure sensors estimating the grip force applied by humans. In a first set of experiments, we ask subjects to mimic a given force profile applied by the robot hand, to understand how human perceive and are able to reproduce a handshaking force. Using the obtained results, we implement three different handshaking controllers, in which we varied the intrinsic and extrinsic contributions and in a second set of experiments, we ask participants to evaluate them in a user study. We show that a sensorimotor delay mimicking the reaction time of the central nervous system is beneficial for making interactions more human-like. Moreover, we demonstrate that humans exploit closed-loop control for handshaking. By varying the controller we show that we can change the perceived handshake quality and also influence personality traits attributed to the robot. ","slug":"icra-handshake","tags":["handshake","icra"],"title":"The Role of Closed-Loop Hand Control in Handshaking Interactions","url":"https://ieeexplore.ieee.org/document/8613850","venue":"2019 IEEE International Conference on Robotics and Automation (ICRA)","date":"Jan 2019"}]}},"pageContext":{}},"staticQueryHashes":["2504448803","3649515864"],"slicesMap":{}}